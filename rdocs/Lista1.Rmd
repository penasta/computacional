---
title: ''
author: ''
date: ''
output:
  pdf_document: null
  fig_crop: no
  html_document:
    df_print: paged
subtitle: ''
highlight: tango
number_sections: no
fig_caption: yes
keep_tex: yes
includes:
  in_header: Estilo.sty
classoption: a4paper
always_allow_html: yes
---
  
  
\begin{center}
{\Large
  DEPARTAMENTO DE ESTATÍSTICA} \\
\vspace{0.5cm}
\begin{figure}[!t]
\centering
\includegraphics[width=9cm, keepaspectratio]{logo-UnB.eps}
\end{figure}
\vskip 1em
{\large
  `r format(Sys.time(), '%d %B %Y')`}
\vskip 3em
{\LARGE
  \textbf{Lista 1}} \\
\vskip 5em
{\Large
  Prof. Dr. Donald Matthew Pianto} \\
\vskip 1em
{\Large
  Aluno: Bruno Gondim Toledo} \\
\vskip 1em
{\Large
  Matrícula: 15/0167636} \\
\vskip 1em
{\Large
  Estatística Computacional} \\
\vskip 1em
{\Large
  1º/2023} \\
\vskip 1em
\vskip 1em
\end{center}

\newpage

```{r setup, include=T, echo=T}

if (!require("pacman")) install.packages("pacman")
p_load(knitr,tidyverse,doParallel,furrr,tictoc,ggpubr)

cores <- detectCores()

```

# Questão 1

```{r dados1, include=T, echo=T}
dicionario <- read_csv("dados/Dicionario.txt", 
    col_names = FALSE)

colnames(dicionario) <- "palavras"
```

## a)

### Resultado analítico:

```{r q1analiticamente, echo=T}

# Separando apenas as palavras com exatamente 5 letras

pcl <- dicionario %>%
  filter(str_length(palavras) == 5)%>%
  mutate(palavras = tolower(str_replace_all(iconv(palavras, "latin1", "ASCII//TRANSLIT"), "[^[:alnum:]]", "")))

# Como o exercicio cita que iremos trabalhar apenas com combinações de letras sem inclusão de caracteres especiais, optei por remover as palavras que são idêntias à outras salvo pela acentuação

pcl <- pcl %>%
  distinct()

# 5481-5427

# Com isso, estaremos trabalhando com 54 palavras a menos (palavras idêntias à outras, salvo por um caracter especial.)

# Certificando a quantidade de letras para calcular a permutação
letras <- dicionario %>%
  mutate(letras = tolower(str_sub(palavras, 1, 1)))%>%
  select(letras)%>%
  distinct()

# Então, sendo 26 letras diferentes, a quantidade de combinações possíveis com 5 letras é da ordem de:

# 26*26*26*26*26

# Ou seja,

# 26^5

# Como são 5427 palavras válidas de 5 letras, então a probabilidade de se obter uma combinação válida é de:

prob <- 5427/(26^5)

prob

# 0.0004567653, ou seja, 0,04567653%.

letras$prob <- seq(1, 26, by = 1) / 26


```

### Resultado com processos iterativos:

```{r q1montecarlo, warning=FALSE,echo=T,cache=TRUE}

probabilidade <- function(){
  contador <- 0
  existe <- FALSE
  while (existe == FALSE) {
    palavra <- NULL
    for (i in 1:5){
      u <- runif(1)
      indice <- which.min(abs(letras$prob - u))
      letra <- letras$letras[indice]
      palavra <- paste0(palavra,letra)
      }
    existe <- any(grepl(palavra, pcl$palavras))
    contador <- contador+1
    }
  rm(u,indice,i,letra,existe,palavra)
  contador
  }

# probabilidade()
# CF <- mean(map_dbl(1:5, ~probabilidade()))

# Como minha função ficou pouco otimizada, vamos paralelizar para melhorar um pouco a velocidade, permitindo um pouco mais de iterações.

# Testando as alternativas de paralelização

# tic()
# plan(multicore)
# CF <- mean(future_map_dbl(1:10, ~probabilidade()))
# toc()

#tic()
#plan(multisession, workers = cores)
#CF <- mean(future_map_dbl(1:10, ~probabilidade()))
#toc()

# O multisession foi superior; aumentando o nível de iterações para a forma final:

plan(multisession, workers = cores)
CF <- future_map_dbl(1:1000, ~probabilidade())

1/mean(CF)

df <- data.frame(iteracao = 1:1000, probabilidade = sapply(CF, function(x) 1/x))
df$media_acumulada <- cumsum(df$probabilidade) / seq_along(df$probabilidade)
df$valor_real <- prob*10

ggplot(df, aes(iteracao, media_acumulada)) +
  geom_line() +
  geom_hline(yintercept = 0.004567653, linetype = "dashed", color = "red") +
  xlab("Iteração") +
  ylab("Valor estimado") +
  ggtitle("Convergência do valor estimado") +
  scale_y_log10()

# Pedi ao ChatGPT que otimizasse minha função. Este foi o output:

probabilidade2 <- function(){
  contador <- 0
  existe <- FALSE
  while (!existe) {
    palavra <- paste0(sample(letras$letras, 5, replace = TRUE), collapse = "")
    existe <- any(pcl$palavras == palavra)
    contador <- contador + 1
  }
  contador
}

# probabilidade2()

#tic()
#plan(multisession, workers = cores)
#CF <- mean(future_map_dbl(1:10, ~probabilidade2()))
#toc()


# De fato, roda bem mais rápido.

# Testando a convergência desta:
# (como ela é mais eficiente, dá para fazer mais iterações)
plan(multisession, workers = cores)
CF2 <- future_map_dbl(1:2000, ~probabilidade2())
1/mean(CF2)

df2 <- data.frame(iteracao = 1:1000, probabilidade = sapply(CF2, function(x) 1/x))
df2$media_acumulada <- cumsum(df2$probabilidade) / seq_along(df2$probabilidade)
df2$valor_real <- prob*10

ggplot(df2, aes(iteracao, media_acumulada)) +
  geom_line() +
  geom_hline(yintercept = 0.004567653, linetype = "dashed", color = "red") +
  xlab("Iteração") +
  ylab("Valor estimado") +
  ggtitle("Convergência do valor estimado")

```

## b)

### Resultado analítico:

```{r q1banalitico, echo=T}

# A quantidade de pseudo-palíndromos possíveis para uma palavra de 5 letras é de

# 26*26*26*1*1
# ou:
# 26^3
# = 17576

# Visto que as 3 primeiras letras podem ser quaisquer, mas a penúltima deve ser igual a segunda (1 combinação possível) e a última deve ser igual a primeira (1 combinação possível).

# Ou seja, das 26^5 combinações possíveis, apenas 26^3 serão palíndromos;

(26^3)/(26^5)
# = 0.00147929

```
### Resultado iterativo:

```{r q1bmontecarlo, echo=T,warning=F,cache=T}

# Para isso, tentarei aproveitar minha última função (a que eu fiz, não a do ChatGPT), adaptando alguns pontos.

palindromo <- function(){
  contador <- 0
  check <- FALSE
  while (check == FALSE) {
    palavra <- NULL
    for (i in 1:5){
      u <- runif(1)
      indice <- which.min(abs(letras$prob - u))
      letra <- letras$letras[indice]
      palavra <- paste0(palavra,letra)
      }
    espelho <- paste0(strsplit(palavra, "")[[1]][5:1], collapse = "")
    check <- palavra == espelho
    contador <- contador+1
    }
  rm(u,indice,i,letra,check,palavra)
  contador
  }

# palindromo()

#tic()
plan(multisession, workers = cores)
CFb <- mean(future_map_dbl(1:2000, ~palindromo()))
#toc()
1/CFb


```

## c)

```{r q1c, warning=FALSE, cache=TRUE, echo=T}

# Definindo vogais e consoantes:
vogais <- c("a","e","i","o","u")
consoantes <- letras$letras[-c(1,5,9,15,21)]

# Construindo a função
gerador <- function(){
  contador <- 0
  check <- FALSE
  while(check==FALSE){
    letra <- sample(letras$letras,1)
    ifelse(letra %in% consoantes, letra2 <- sample(vogais,1),letra2 <- sample(consoantes,1))
    ifelse(letra2 %in% consoantes, letra3 <- sample(vogais,1),letra3 <- sample(consoantes,1))
    ifelse(letra3 %in% consoantes, letra4 <- sample(vogais,1),letra4 <- sample(consoantes,1))
    ifelse(letra4 %in% consoantes, letra5 <- sample(vogais,1),letra5 <- sample(consoantes,1))
    palavra <- paste0(letra,letra2,letra3,letra4,letra5,sep="")
    rm(letra,letra2,letra3,letra4,letra5)
    check <- any(palavra %in% pcl$palavras)
    contador <- contador+1
    }
  check <- FALSE
  contador
}

# gerador()

# Iterando a função algumas vezes:
#tic()
plan(multisession, workers = cores)
CF3 <- mean(future_map_dbl(1:2000, ~gerador()))
#toc()
# Resultado:
1/CF3

```

## d)

```{r q1d_dados, include=T, echo=T}

# Definindo o vetor de probabilidades, segundo as informações da Wikipedia
freq <- read_delim("dados/freq.txt", 
    delim = "\t", escape_double = FALSE)
colnames(freq) <- c("letra","freq")
```


```{r q1d, warning=FALSE, cache=TRUE, echo=T}

freq$freq <- gsub("%", "", freq$freq)
freq$freq <- as.numeric(freq$freq)
freq$freq <- freq$freq/100

# Amostrando:

acabou_a_criatividade_para_nome_de_funcao <- function(){
  contador <- 0
  check2 <- FALSE
  while(check2 == FALSE){
    letras <- sample(x=freq$letra,size=5,prob = freq$freq, replace = TRUE)
    palavra <- paste0(letras,collapse="")
    check1 <- grepl("a", palavra)
    if(check1 == TRUE){
      check2 <- palavra %in% pcl$palavras
      contador <- contador+1
      }
    }
  contador
  }

# acabou_a_criatividade_para_nome_de_funcao()

# Iterando a função algumas vezes:
#tic()
plan(multisession, workers = cores)
CF4 <- mean(future_map_dbl(1:2000, ~acabou_a_criatividade_para_nome_de_funcao()))
#toc()
# Resultado:
1/CF4

```

Quanto à demonstração analítica, *Cuius rei demonstrationem mirabilem sane detexi hanc marginis exiguitas non caperet*

# 2)

## a)

```{r q2a, echo=T}
# usando a função quantílica (assim como no caso da exponencial do slide); e fixando gama=1;
n<-1000
u<-runif(n)
z <- tan(pi * (u - 0.5))
hist(z)

#comparando com a distribuição "real"

hist(rcauchy(1000))

# Aparenta estar correto..

```


## b)

```{r q2b, echo=T}

# Usando a runif
u <- runif(1000)
x <- case_when(
  u < 0.2 ~ 2,
  u >= 0.2 & u < 0.3 ~ 3,
  u >= 0.3 & u < 0.5 ~ 5,
  u >= 0.5 & u < 0.7 ~ 7,
  u >= 0.7 ~ 9)

# Gerando a tabela de frequências relativas observadas; após inserindo a frequência esperada
df <- data.frame(factor(x)) %>%
  rename(valor = 1)%>%
  group_by(valor) %>%
  summarise(n=n(),freq_observada = n/1000)
df$freq_esperada <- c(.2,.1,.2,.2,.3)

# Usando a função sample:
df2 <- sample(x=c(2,3,5,7,9),size=1000,prob=c(.2,.1,.2,.2,.3),replace=T)

# Criando a tabela dos numeros gerados pela segunda função

df2 <- data.frame(factor(df2)) %>%
  rename(valor = 1)%>%
  group_by(valor) %>%
  summarise(n=n(),freq_observada = n/1000)
df2$freq_esperada <- c(.2,.1,.2,.2,.3)

kable(df)
kable(df2)


```

## c) 

```{r q2c, warning=FALSE, cache=TRUE, echo=T}

# Irei tentar implementar o método de Rejeição de Von Neumann, tentando gerar valores de uma distribuição x a partir de uma distribuição y. No caso, x será a normal padrão e y será a cauchy padrão.

f <- function(x) dnorm(x, 0, 1)
g <- function(x) dcauchy(x, 0, 1)

C <- sqrt(2 * pi/exp(1)) # Ajustando uma constante C para ajustar a escala da distribuição cauchy para que ela cubra a distribuição normal padrão.

N <- 5000 # Definindo o número de observações
x <- numeric(N)
i <- 1

while (i <= N) {
  y <- rcauchy(n = 1)
  u <- runif(n = 1)
  if (u < f(y)/(C * g(y))) {
    x[i] <- y
    i <- i + 1
  }
}

qqnorm(x)
ggdensity(x)
shapiro.test(x)

# Os testes não rejeitam a hipótese nula de normalidade da variável.

# Fazendo a versão paralelizada, para testar com um conjunto maior de pontos.

set.seed(150167636)
#tic()
x <- future_map_dbl(.x = 1:100000, ~{
  y <- rcauchy(n = 1)
  u <- runif(n = 1)
  if (u < dnorm(y, 0, 1)/(C * dcauchy(y, 0, 1))) {
    y
  } else {
    NA_real_
  }
})
x <- na.omit(x)
#toc()

# Neste caso não é possível executar o teste Shapiro-Wilk pela quantidade de
# observações ser muito grande. Então ficaremos apenas com as representações visuais

qqnorm(x)
ggdensity(x)

# Pelos gráficos Q-Q e densidade observada, aparenta ter funcionado.

```

